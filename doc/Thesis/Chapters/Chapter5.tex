\chapter{FPGA Implementation} % Main chapter title
\label{Chapter5}
%\epigraph{The key to artificial intelligence has always been the representation.â€ }{\textit{Jeff Hawkins}}

\section{Tools Used}
The CNN (Inference) Hardware Accelerator was implemented using the Xilinx Vivado Design Suite - HL System Edition 2017.1. Vivado Design Suite is a software suite developed by Xilinx for synthesis and analysis of HDL designs, superseding Xilinx ISE with additional features for system on a chip development and high-level synthesis. The tools used are the Vivado HLS, Vivado IDE, and Xilinx SDK \cite{Link1}.

\subsection{Vivado HLS}
The Xilinx Vivado HLS tool \cite{Link3} provides a higher level of abstraction for the user by synthesizing functions written in C,C++ (with constraints and feautures) into IP blocks, by generating the appropriate ,low-level, VHDL and Verilog code. Then those blocks can be integrated into a real hardware system. Vivado HLS is tightly integrated with the rest of the Xilinx design tools and provides comprehensive language support and features for lower level optimizations, making it possible to optimize the C code for hardware systems.
The Vivado HLS tool allows for C functions written in C, C++, SystemC, or an OpenCL API C kernel. We decided to use C++, as there are several supported features in HLS design (template e.t.c.) . To debug the code, Vivado HLS uses a C test bench to simulate the C function prior to synthesis and to verify the RTL output using C/RTL Cosimulation.

The tool also adds some constraints to the exported IP block, like the clock period, clock uncertainty, and FPGA target. The clock uncertainty defaults to 12.5\% of the clock period, but you have the option to change it. Finally, the tool allows for directives to be added to direct the synthesis process to implement a specific behavior or optimization. Directives are optional and do not change the behavior of the c code in the simulations, only in the synthesized IP block.  

\subsubsection{Synthesis Report in Vivado HLS}

When synthesize an HLS code tool exports a synthesis report showing the performance metrics of the generated design. The report's information on performance metrics are presented below:
\begin{itemize}
  \item \textbf{Area:} Amount of hardware resources required to implement the design based on the resources available in the target FPGA. The types of resources are, Look-Up Tables (LUT), Flip Flops (FF) , Block RAMs (BRAMs), and DSP48s.
  \item \textbf{Latency:} Number of clock cycles required for the function to compute all output values.
  \item \textbf{Iteration Interval (II):} Number of clock cycles before the function can accept new input data.
  \item \textbf{Loop Initiation Interval:} Number of clock cycle before the next iteration of the loop starts to process data.
   \item \textbf{Loop Latency:} Number of cycles to execute all iterations of the loop.
  \item \textbf{Loop Iteration Latency:} Number of clock cycles it takes to complete one iteration of the loop.
\end{itemize}


\subsubsection{Optimizations in Vivado HLS}
 Vivado HLS also provides (optional) directives that can be used to optimize the design: reduce latency, improve throughput performance, and reduce area and device resource utilization of the resulting RTL code. These pragmas can be added directly to th1e source code for the kernel.  
\begin{itemize}

    \item \textbf{Pipeline}: The PIPELINE pragma reduces the initiation interval for a function or loop by allowing the concurrent execution of operations. A pipelined function or loop can process new inputs every N clock cycles, where N is the initiation interval (II) of the loop or function. 
    
    \item \textbf{Array Partition}: Partitions an array into smaller arrays or individual elements.
    
This partitioning:
\begin{itemize}
    \item Results in RTL with multiple small memories or multiple registers instead of one large memory.
    \item Effectively increases the amount of read and write ports for the storage.
    \item Potentially improves the throughput of the design.
    Requires more memory instances or registers.
\end{itemize}

    
    \item \textbf{Unroll}: Unroll loops to create multiple independent operations rather than a single collection of operations. The UNROLL pragma transforms loops by creating multiples copies of the loop body in the RTL design, which allows some or all loop iterations to occur in parallel. 
 
    \item \textbf{Stream}:By default, array variables are implemented as RAM. If the data stored in the array is consumed or produced in a sequential manner, a more efficient communication mechanism is to use streaming data as specified by the STREAM pragma, where FIFOs are used instead of RAMs.

    \item \textbf{Array Map}: Combines multiple smaller arrays into a single large array to help reduce block RAM resources.

    \item \textbf{Resource}: Specify that a specific library resource (core) is used to implement a variable (array, arithmetic operation or function argument) in the RTL. If the RESOURCE pragma is not specified, Vivado HLS determines the resource to use.    

    \item \textbf{Dataflow}: The DATAFLOW pragma enables task-level pipelining, allowing functions and loops to overlap in their operation, increasing the concurrency of the RTL implementation, and increasing the overall throughput of the design.\newline \newline
    
    
\end{itemize}



\subsection{Vivado IDE}

The Vivado IDE ( was implemented from  co-partner Giannis Kalomoiris) is the GUI for the Vivado Design Suite. All of the Vivado design Suite tools are written with a native Tcl interface, and all of those commands are available through the IDE either through the GUI or through the Tcl console. Tcl commands can be entered in the Tcl Console in the Vivado IDE or using the Vivado Design Suite Tcl shell. You can run analysis and assign constraints throughout the design process. Timing and power estimations are provided after synthesis, placement, and routing. 
 



\subsection{Vivado SDK}

The Xilinx SDK is an IDE tool to develop embedded software applications targeted towards Xilinx ARM processors. The SDK works with hardware designs and bitstreams created with Vivado Design Suite. The SDK is based on the Eclipse standard. SDK includes an  C/C++ code editor and a compilation environment with easy project management, application build configuration and automatic Makefile generation. It also provides an environment for debugging and profiling of software code as well as design performance. The SDK also provides it own tools to configure FPGAs and create bootable bitstrems with software extensions.
We open the SDK (through IDE) using the preconfigured directories, including bitsreams after successful Vivado IDE implementation. The SDK automatically imports the project hardware wrapper and generates the files needed(memory porting , defines ,etc) for the software part. Therefore we create a new fuzzy project which generates the project files,and the needed BSP packages, which includes the suitable drivers for the software design that the PS has access to.

The SDK is fisrtsly used to create a simple program to run by the PS to test and debug the PL functionality. To be able to program the FPGA, the JTAG port has to be connected to the PC, and to monitor and debug it we use the UART port as well. Another very useful tool that is part of the Vivado IDE is the Hardware Manager. It connects to the ILAs that have been added to the Vivado project and allows us to monitor in real-time the values of the signals between our modules, while the program is running.

For the needs of the SDK, we have created functions where activate and initialize our IPs, DMA, memories, and caches. Additionally, functions for writing and reading data from the SD card were implemented. We made a study to be able to measure the real memory bandwidth. The data was stored in the DDR in 2 ways either by SD read or JTAG. Finally, time measurement functions were used to enable speedups to be evaluated.




\section{FPGA platforms}

Our architectures are targetting on 2 FPGA platforms we worked on.
\subsection{Xilinx Zynq UltraScale+ MPSoC ZCU102}

The ZCU102 is a general purpose evaluation board for rapid-prototyping based on the Zynq UltraScale+ XCZU9EG-2FFVB1156E-2-i MPSoC (multiprocessor system-on-chip). High-speed DDR4 SODIMM and component memory interfaces, FMC expansion ports, multi-gigabit per second serial transceivers, a variety of peripheral interfaces, and FPGA logic for user customized designs provides a flexible prototyping platform.

Below in table \ref{tab:7} we present main features of the FPGA


\begin{table}[h]
\captionof{table}{ZCU102 Specifications} \label{tab:7} 
\centering
\begin{tabular}{l l l l l l}
\toprule
\tabhead{Logic Cells} & \tabhead{B-RAM} & \tabhead{DSP Slices} & \tabhead{PS DDR} & \tabhead{PL DDR} \\
\tabhead{(K)} & \tabhead{(MB)} & \tabhead{} & \tabhead{(GB)} & \tabhead{(MB)} \\
\midrule

600 & 4  & 25200 & 4 & 512\\

\bottomrule
\end{tabular}\par
\end{table}
\begin{center}
MB=Mbytes , GB=Gbytes
\end{center}


\subsection{QFDB}
QFDB is a 4-FPGA custom platform designed and implemented by FORTH. It has 4 ZCU102 FPGA,(meaning 4x powerful than ZCU102) thus our architecture transference between platforms are fully compatible. 

\section{Bottom-up Strategy}
The first approach was implemented with the bottom-up strategy. A bottom-up approach is the unification of many "simple" systems to end up in a more "complex" system.



%%%%%%%% /////////////eikona


The algorithm was divided into small entities with specific behavior and hierarchical structure. Then they would be joined together to make the overall accelerator. The entities were separated as follows:
\begin{itemize}

    \item \textbf{Convolution (1-D) }(conv1): $1^{st}$ Convolutional layer of the network (Input: image, kernel1, bias1)(Output:ConvStage1)
    \item \textbf{Convolution (2-D) }(conv2): $2^{nd}$ Convolutional layer of the network (Input: ConvStage1, kernel2, bias2)(Output:ConvStage2)
    \item \textbf{Convolution (2-D) }(conv3): $3^{rd}$ Convolutional layer of the network (Input: ConvStage2, kernel3, bias3)(Output:ConvStage3)
    \item \textbf{Fully Connected }(fc):  Fully Connected of the network (Input: ConvStage3, $kernel_{dense}$, $bias_{dense}$)(Output:Classification) 

    \item \textbf{ReLU} : ReLU is the main activation function that was performed after every convolutional layer. For this purpose, ReLU was inserted in every Convolutional Layer entity.
\end{itemize}


\subsection{Two Stage Architecture}
After the network was divided and depicted in small entities, they were reunited to make the two basic modules of our architecture. The first module is  "Convolutional Layers" where it contains the three Convolutional Layer with the corresponding ReLU. The second module is the "Fully Connected Layer" where it receives the exit of the first module and makes the final classification.
This leads to a 2-Stage Architecture designed to communicate 2 FPGAs. The first will implement Convolution and the second the Fully Connected module \ref{fig:21}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Images/datapath_2Stage.png} 
\decoRule
\caption[Datapath of Two-Stage Architecture]{Datapath of Two-Stage Architecture
}
\label{fig:21}
\end{figure}

\section{First Approach}

\subsection{Memory and I/0}
Firstly we have to clarify how to insert the data into the different Building Blocks of the accelerator from the outside world. The first step is to pass the data to the DDR of the processor. Then we have three ways to link this data to FPGA:
\begin{itemize}

    \item \textbf{Memory-mapped I/O }(MMIO): A complementary method of performing input/output (I/O) between the central processing unit (CPU) and peripheral devices such as FPGA. This method uses the same address space to address both memory and I/O extensions. The memory and registers of the I/O devices are mapped to (associated with) address values. This technique comes to emphasize the main DDR drawback, which is that it can not efficiently drive multiple requests because it is random access. This means that for each request we would have to "pay" the initial interval (30 -50 cycles) and so there would be no meaning for pipeline design in the FPGA. This technique can be implemented through using Xilinx's IP Data-movers.
    
    \item \textbf{Streaming }(AXI-4): The second method we can use is Streaming Interface using AXI-4 protocol. In fact, we are creating a continuous DDR communication channel with FPGA (a large FIFO) and we are sending the processor the data we want without requests. This relieves us from the delay of requests, and you create a huge pipeline for entering data by hiding the DDR interval. This technique can be implemented through using Xilinx's IP DMA. 
    
    \item \textbf{B-RAM }: Another approach is to transfer the data in burst with memory mapped or stream and then store them in B-RAM to take advantage of the huge bandwidth. In order to achieve this, the data must have a small memory footprint. Typically the B-RAM is in the order of many KBs and a few MBs. \newline \newline
    
    
\end{itemize}

Using Streaming Interface is much more efficient because it allows us to fully utilize the DDR bandwidth (about 10 GBs). However, this method can be used only in cases where we know in advance the data-flow. CNNs have inherently streaming nature, so they are ideal for taking advantage of this.

\subsection{Transference to HLS}

A next step was to be able to integrate each Entity's algorithm parts from MATLAB into C++ and then into HLS synthesizable code. In order to successfully complete this, the limitations and capabilities of the tool had to be studied. Convolution was the most algorithmic complex part. On the other hand, Matrix Multiplication has less algorithmic complexity, but it requires much more computation time.

\subsection {Convolution (1-D)}
First, we grant the weights and the image into the FPGA and store them in the internal B-RAM. This approach came because we need to use that data several times. The reasoning in the use of B-RAM instead of sending the data with Stream interface was to be able to exploit its bandwidth (TBs).

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Images/Convolution1_Datapath.png} 
\decoRule
\caption[Datapath of Convolution(1-D)]{Datapath of Convolution(1-D)}
\label{fig:22}
\end{figure}


\subsubsection{Array Partitioning}
B-RAM has memory access limitations because it has two memory channels. A feature that HLS provides you is the option to partition a B-RAM array. This allows you to have more than two memory accesses in a clock cycle. Essentially it creates copies of your original array with the use of multiplexer systems. The nature of the algorithm is access to specific arrays many times in the same cycle, so it is advisable to use this directive.

\subsubsection{Pipeline}
In FPGA designing, it is very important to manage running your algorithm in a large pipeline, thus taking advantage of FPGA's capabilities. However, in order to succeed Pipeline, a limitation that came up was algorithmic. We basically wanted to process an 8-part set, e.g. 0-7, of the image in a cycle. In the next cycle, we have to retain the last 7 parts and store the next part, having at the end of this cycle a new 8-part set e.g. 1-8 (stride=1 of the network). This was approached by creating a new dimension in the image and implementing it as a shifted B-RAM, achieving by that pipeline=1.

%%%%%%%%%%%%%sxima 1-7 2-8
%%%%%%%%%%%%%
\subsection {Convolution (2-D)}

The complexity of this entity is 16x compared to Convolution(1-D), as it has an extra dimension size of 16. The weights and the image are granted into the FPGA and store them in the internal B-RAM with the same way. This approach came because we need to use that data several times. The reasoning in the use of B-RAM instead of sending the data with Stream interface was to be able to exploit its bandwidth (TBs).

\subsubsection{Array Partitioning}
B-RAM has memory access limitations because it has two memory channels. A feature that HLS provides you is the option to partition a B-RAM array. This allows you to have more than two memory accesses in a clock cycle. Essentially it creates copies of your original array with the use of multiplexer systems. The nature of the algorithm is access to specific arrays many times in the same cycle, so it is advisable to use this directive.

\subsubsection{Pipeline}
In order to succeed Pipeline a limitation that came up was also algorithmic. We basically wanted to process 16x8 parts of the image in a cycle and in the next cycle to retain the last 16x7 and store the next 16 part (stride=1 of the network). This was approached by creating a new dimension in the image and implementing it as a shifted B-RAM, achieving by that an overall pipeline=8.The time-chart of Convolutional Layers presents in figure \ref{fig:conv_simple}.

 \begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Images/Conv_simple.png} 
\decoRule
\caption[Convolutional Layers Time-Chart (FA)]{Convolutional Layers Time-Chart of First Approach}
\label{fig:conv_simple}
\end{figure}

\subsection {Fully Connected}
The idea of the streaming interface was fully utilized to implement Fully Connected Layer. We store into B-RAM the least possible data(image stage, bias) and we basically stream the weights from DDR. The reason for this is because weights of the dense layers could not be stored into B-RAM. \ref{fig:fc_mod}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Images/Fully_Connect.png} 
\decoRule
\caption[Fully Connected Module]{Fully Connected Module}
\label{fig:fc_mod}
\end{figure}

\subsubsection{Pipeline}
A Pipeline (Iteration Interval=1) has been implemented essentially by export each partial result in each cycle. The time-chart of CNN presents in figure \ref{fig:cnn_simple}.


\begin{table}[h]
\caption{First Approach Performance}
\label{tab:1}
\centering
\begin{tabular}{l l l l}
\toprule
\tabhead{Modules} & \tabhead{Latency} & \tabhead{Comp. Performance} &\tabhead{Bandwidth} \\
\tabhead{} & \tabhead{(cycles)} & \tabhead{(GFLOPS)} &\tabhead{(GB/s)} \\
\\
\midrule
conv & 593596 & 7.6   & 1\\
dense & 22971200  & 0.6   & 1\\
conv+dense & 23564796  & 0.77  & 1\\
\bottomrule
\end{tabular}
\end{table}

\begin{center}
GB=Gbytes
\end{center}

In computing, floating point operations per second (FLOPS, flops or flop/s) is an important measure of computational performance, useful in fields of scientific computations that require floating-point calculations \ref{eq:FLOPs}.

\begin{equation}\label{eq:FLOPs}
FLOPS=\frac{cycles}{second}*\frac{FLOPs}{cycle}
\end{equation}


 \begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Images/FCNN_simple.png} 
\decoRule
\caption[CNN Time-Chart (FA)]{CNN Time-Chart of First Approach}
\label{fig:cnn_simple}
\end{figure}

\section{A better Approach}
After the first approach was realized how long we were from a possible speedup over the GPU. The main obstacle was that we did not use the maximum DDR Bandwidth (we used 1GB / s, meaning 6\% of the maximum theoretical Bandwidth), resulting in the failure to exploit the processing power of the FPGA.

\subsection{Larger Streaming Buses}
The next action is to try to limit ourselves from the bandwidth of DDR. Consequently, we tried to use larger memory streaming buses (more than 32bits). The limitations here are three:

\begin{itemize}

    \item \textbf{DMA}: The maximum memory bus that DMA can support is 1024 Bits in one cycle.
     
     \item \textbf{DDR Bandwidth}: The maximum bandwidth of the DDR is 16GB/s. So the maximum bus considering 300MHz clock we can support is 458 bit from \ref{eq:8}.
    \item \textbf{HP ports }: The maximum memory bus that the CPU can transfer from memory to FPGA through DMA. This is done through HP ports and the limit is 128-bit. \newline \newline
    
\end{itemize}

\begin{equation}\label{eq:8}
  Bus_{BitWidth}= \frac{Bandwidth}{Clock_{frequency}}
  \end{equation}
  \begin{center}
       \small{Bandwidth=Memory Bandwidth in b/s , $Clock_{frequency}$ of the FPGA} \newline 
  \end{center}

Therefore we end up using 128 bits, that is 4 times larger than the previous ones. This means we use 4 times more memory bandwidth. This could lead to a 4x speedup by implementing the ideal architecture. 

\subsection{Multiple DMAs}
Next, we realize that even though we used 4GB/s bandwidth we have not reached its full potential(16GB/s). The next solution we can exploit is to use multiple DMAs where they will stream from different HP ports (maximum 6) data linking in the same DDR. Theoretically, 4 DMAs is the golden ratio because we will be totally restrained from the memory. 

\subsubsection{DDR Bandwidth}
In order to be convinced how many DMAs (and HP ports) would be the best to use we did some fuzzy IPs (which reads and writes random values through DMAs) to be able to accurately count read, write and read/write bandwidth as showing table \ref{tab:8}. Furthermore to achieve higher bandwidth we enabled caches and flushed them with the weights.

\begin{table}[h]
\captionof{table}{Memory Bandwidth} \label{tab:8} 
\centering
\begin{tabular}{l l l l l }
\toprule
\tabhead{HP ports} & \tabhead{Read BW} & \tabhead{Write BW} & \tabhead{Read/Write BW}  \\
\tabhead{} & \tabhead{(GB/s)} & \tabhead{(GB/s)} & \tabhead{(GB/s)}  \\

\midrule

1 & 4.15  & 4.15 & 4.18  \\
2 & 8.24  & 8.23 & 8.3  \\
3 & 9.8  & 9.9 & 12.4 \\
4 & 10.2  & 10.4 & 16.8 \\
\bottomrule
\end{tabular}\par
\begin{center}
GB=Gbytes
\end{center}
\end{table}



Read and Write channels are separated, so it makes sense not to see a linear increase in bandwidth for reading and write separately. Whereas when we use both channels we see linearity.
Our algorithm is using the read channel much more because it requires to read more data than to write in the final classification. Therefore we conclude that the best possible case for not consuming resources is to use 2 HP ports and 2 DMAs.


\subsection {Convolution (1-D) and Convolution (2-D)}

Having 8 times higher bandwidth than the first approach we can store B-RAM 8 times faster. The rest and most of the implementation reads the data through B-RAM, so there is no significant optimization.



\subsection {Fully Connected}

On the other hand, the fully connected was clearly set up as streaming oriented IP. Now by bringing 8 times more data (2 DMA (128) -> 256 bit versus 32bit), we must take advantage of this feature and come up with  8x speedup relation to the first approach.

\paragraph {Multiply and Accumulate}
To be able to achieve this we need to observe from a different perspective the basic operation of multiply and accumulate and try to "hide" the Latency of the 10 cycles it needs. In order to be able to exploit the rate that weights are being streamed in, we have to increase the parallelism at the level of operations.

\paragraph{First Approach} Trying to make the 8-fold multiplication and addition based on the current Fully Connected implementation end up in a Pipeline equal to the accumulative delay (10 in our case), something that does not satisfy us \ref{alg:MAC_1}. This leads to a very poor exploit of the memory bandwidth. It is perceptible that we have to follow a different implementation trying to achieve a full pipeline module, managing to hide the MAC operation delay.


\begin{algorithm}[h]
\caption{Matrix Multiplication}\label{alg:MAC_1}
\begin{algorithmic}[1]
\Procedure{Matrix Multiplication}{$input,weights$}  
\State $NumOfClasses \gets size(weights,1)$ \Comment{Number of Classes}
\State $NumOfWeights \gets size(weights,2)$ \Comment{Number of Weights/Class}
 \State $FaddLat=10$ \Comment{Latency of Multiply and Accumulate}
\For{i:=1 \textbf{to} NumOfClasses }     
\State $Classes(i)\gets 0$
\EndFor 
 \State $sum\gets 0$

\For{i:=1 \textbf{to} NumOfClasses }

\State $sum=0$
\For{j:=1 \textbf{to} NumOfWeights }           
\State $ PIPELINE=FaddLat$
\State $sum \gets sum + input(j)*weights(i,j)$
\EndFor 
\State $Classes(i)\gets sum$
\EndFor 

\State \textbf{return} $Classes$       
\EndProcedure
\end{algorithmic}
\end{algorithm}

\paragraph {Better Approach}To achieve that we will perform the task in a different way. In fact, we will calculate different partial results in parallel for each class. We will end up having the partial results that need to be added to produce a result. This will be achieved by creating a deep adder tree. The appropriate number of different partial results must be equal to the adder delay, meaning 10 in \ref{alg:MAC_2}.

\paragraph {Cyclic Array}
We originally solved the problem of how we process the 8-fold weights in a single clock cycle and exploit the resources to do the 8-fold multiplication and addition. Instantly the problem that arises is to explore how we will ensure access to 8-Images stages of the edited image (in a cycle). This could easily be achieved using the array partition of the tool by applying a cyclic partition. This allows us, depending on the factor we will set, in how many consecutive cells of the array we will have access ( in a cycle). However, because the array was large (28464), the tool used a large multiplexer system to route access to the memory and thus did not allow us to reach a competent clock (it greatly increased the critical path).

\paragraph {Custom Cyclic Array}
So we ended up making our own "cyclic partition" helping the tool to achieve the behavior we wanted. To achieve this we created 4 subterranean arrays of the original one. We created 4 and not 8 because each B-RAM has 2 memory channels, meaning 2 memory accesses. We stored weight1,weight2 in the first table, weight3,weight4 in the second etc. in order to be able to have access in 8 consecutive image-stages. Hence we use a same sized B-RAM, but instead of reading from 1 array, we now read from 4 different subterranean arrays by striking the behavior we wanted. Finally, we ended up creating our own "cyclic partition" helping the tool to achieve the behavior we wanted.

\begin{algorithm}[h]
\caption{Matrix Multiplication (Opt)}\label{alg:MAC_2}
\begin{algorithmic}[1]
\Procedure{Matrix Multiplication}{$input,weights$}  
\State $NumOfClasses \gets size(weights,1)$ \Comment{Number of Classes}
\State $NumOfWeights \gets size(weights,2)$ \Comment{Number of Weights/Class}
\State $FaddLat=10$ \Comment{Latency of Multiply and Accumulate}

\For{i:=1 \textbf{to} NumOfClasses }     
\State $ UNROLL$
\State $Classes(i)\gets 0$
\EndFor 

\For{i:=1 \textbf{to} FaddLat}     
\State $ UNROLL$
\State $partialSum(i) \gets 0$
\EndFor 


\For{i:=1 \textbf{to} NumOfClasses \textbf{step}=FaddLat }
\State $ PIPELINE II=FaddLat$

\For{k:=1 \textbf{to} FaddLat }     
\State $ UNROLL$
\State $partialSum(k)\gets 0$
\EndFor 
\For{j:=1 \textbf{to} NumOfWeights }           

\State $partialSum(j) \gets partialSum(j) + input(j)*weights(i,j)$
\EndFor 

\For{v:=1 \textbf{to} FaddLat }     
\State $UNROLL$
\State $Classes(k)\gets Classes(k)+partialSum(v)$
\EndFor 

\EndFor 

\State \textbf{return} $Classes$       
\EndProcedure
\end{algorithmic}
\end{algorithm}



\begin{table}[h]
\caption{Second Approach Performance}
\label{tab:1}
\centering
\begin{tabular}{l l l l}
\toprule
\tabhead{Modules} & \tabhead{Latency} & \tabhead{Comp. Performance} &\tabhead{Bandwidth} \\
\tabhead{} & \tabhead{(cycles)} & \tabhead{(GFLOPS)} &\tabhead{(GB/s)} \\
\midrule
conv & 593596 & 7.6  & 1\\
dense & 2871400 & 4.8  & 8.23\\
conv+dense & 3464996 & 5.25  & 9.23\\
\bottomrule\\
\end{tabular}
\begin{center}
GB=Gbytes
\end{center}
\end{table}



\section{Architecture v1.0}
In this architecture, we basically integrate the study implemented in Sensitivity Analysis \ref{Chapter4}. We have been able to reduce the memory footprint of weights to a satisfactory degree with small error. Hence we can speed the processing of our data. Then we managed to make our network a huge pipeline from the image entering and communication of Convolution Layers to the communication of the three Convolutional Layers with the Fully Connected Layer. Afterwards, we perform resource optimizations to be able to fit our network into an FPGA.
 
 
 \begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Images/Datapath_Arch1.png} 
\decoRule
\caption[Datapath of the Architecture v1.0]{Datapath of the Architecture v1.0}
\label{fig:27}
\end{figure}

\subsection{Embedding Compressed Weights}
Originally, the analysis was done to reduce the memory footprint of the weights that only applies to the fully connected, because this is the main memory bottleneck of the algorithm. Furthermore, compressed weights are used in the I/O streaming interface during the processing. We used 256-bit channel from the memory (2-DMA of 128 bits) based on the previous analysis on memory buses. Each compressed weight has a 4-bit size. Therefore we can stream 64 weights in a cycle(stream read). This gives us a possibility for a huge parallelism at the operation level. To achieve this we extend the previous architecture to operation level parallelism.

\subsection{Pipelining Convolution Module}
The next step is to try to get the most out of all available resources. To accomplish this, a pipeline must be created between convolutional layers as shown in figure \ref{fig:conv_pipe}, in such a way that different parts of the image are processed at the same time by the three convolutional entities.


To perform this we need to change the way we obtain the data and output them to the next entity in a way that is suitable for it, to process them in parallel with the previous one. FIFO must be placed between the layers. Thus we create a channel in which every entity will be able to obtain image-parts. We need to introduce some new entities with specific behavior that will satisfy this purpose and will enable pipeline processing.


 \begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Images/Pipeline_Con.png} 
\decoRule
\caption[Convolutional Layers Time-Chart (Arch1.)]{Convolutional Layers Time-Chart of Architecture v1.0}
\label{fig:conv_pipe}
\end{figure}

In table \ref{tab:pipe_conv_arch1} we present performance results after the pipeline of Convolutional Layers.


\begin{table}[h]
\caption{Pipeline Convolutional Layers}
\label{tab:pipe_conv_arch1}
\centering
\begin{tabular}{l l l }
\toprule
\tabhead{Modules} & \tabhead{FLOPS} & \tabhead{MAC/cycle}   \\
\midrule
conv1 & 459K & 8 \\
conv2 & 7.3M & 16  \\
conv3 & 7.3M & 16 \\
CONV & 15.1M & 40(peak) \\
\bottomrule\\
\end{tabular}
\begin{center}
GB=Gbytes
MAC = Multiply and Accumulate
FLOPS = Floating Point Operations
\end{center}
\end{table}


\subsubsection{Shifted FIFO}

The utility of this entity is double. Originally, this implements a custom behavior of a Shifted FIFO. It receives data from the previous layer, and when it reaches 16x8 it pushes it in 16 packages and streams them through a FIFO (512 bits) to the next Layer to start the processing. Then in the next cycle, the next module will need the last 16x7 + the new 16 data. Therefore here comes the shift register. At the same time, we do not have to store some of the stages of the image (like before in B-RAM), we just go through FIFOs and read them there. The shift register functionality could be added to the next layer. However, this has led to great critical paths and that's why its behavior has taken place in a different entity.

\subsubsection{Task Level Parallelism}
Having created all the structures to activate the pipeline, the next step is to use the Xilinx dataflow pragma. However, trying to incorporate it, we realized that there was a feature, sequential and parallel processing in different modules at the same time, that was not supported. As a result, when the tool noticed any FIFO or some kind of stream behavior, it was trying to turn it into task level parallelism.

\paragraph{Sequential and Parallel Processing}

Accordingly, we should find a workaround to incorporate this feature. This problem was presented in the insertion of the initial data (image, weights, bias) in the 1st module. Essentially we were reading at several points from the same stream and stored in different B-RAMs. The solution was to create a custom mutual exclusion for reading in this stream (mutex). Therefore we forced the tool to read in a specific manner the information and store it in specific B-RAMs. On the other hand, the image is passed to an internal FIFO to continue the flow in the next modules.
Below in \ref{alg:SP} we present the behavior above.

\begin{algorithm}[h]
\caption{Sequential Parallel with MUTEX}\label{alg:SP}
\begin{algorithmic}[1]
\Procedure{Sequential Parallel with MUTEX}{$InStream$}  
\State $SKernel_{1} \gets size(kernel_{1})$ \Comment{Size of $kernel_{1}$}
\State $SKernel_{2} \gets size(kernel_{2})$ \Comment{Size of $kernel_{2}$}
\State $SKernel_{3} \gets size(kernel_{3})$ \Comment{Size of $kernel_{3}$}
\State $SBias_{1} \gets size(bias_{1})$ \Comment{Size of $bias_{1}$}
\State $SBias_{2} \gets size(bias_{2})$ \Comment{Size of $bias_{2}$}
\State $SBias_{3} \gets size(bias_{3})$ \Comment{Size of $bias_{3}$}
\State $SImage \gets size(Image)$ \Comment{Size of Image}
\State $TotalSize \gets SKernel_{1}+SKernel_{2}+SKernel_{3}+SBias_{1}*3$ \Comment{Total Size}

\For{i:=1 \textbf{to} TotalSize }     
\If{$i\leq SKernel_{1}$}
\State $Kernel_{1}(i) \gets InStream.read$ 
\ElsIf{$i\leq SKernel_{1}+SKernel_{2}$}
\State $k\gets i-SKernel_{1}$ 
\State $Kernel_{2}(k) \gets InStream.read$ 
\ElsIf{$i\leq SKernel_{1}+SKernel_{2}+SKernel_{3}$}
\State $k\gets i-SKernel_{1}+SKernel_{2}$ 
\State $Kernel_{3}(k) \gets InStream.read$ 
\ElsIf{$i\leq SKernel_{1}+SKernel_{2}+SKernel_{3}+SBias_{1}$}
\State $k\gets i-SKernel_{1}+SKernel_{2}+SKernel_{3}$ 
\State $Bias_{1}(k) \gets InStream.read$ 
\ElsIf{$i\leq SKernel_{1}+SKernel_{2}+SKernel_{3}+SBias_{1}*2$}
\State $k\gets i-SKernel_{1}+SKernel_{2}+SKernel_{3}+SBias_{1}$ 
\State $Bias_{2}(k) \gets InStream.read$ 
\ElsIf{$i\leq SKernel_{1}+SKernel_{2}+SKernel_{3}+SBias_{1}*3$}
\State $k\gets i-SKernel_{1}+SKernel_{2}+SKernel_{3}+SBias_{1}*2$ 
\State $Bias_{3}(k) \gets InStream.read$ 
\ElsIf{$i\leq TotalSize$}
\State $StreamImage.Write(InStream.read)$ 
\EndIf
\EndFor 

\State \textbf{return} $Kernel_{1},Kernel_{2},Kernel_{3},Bias_{1},Bias_{2},Bias_{3},StreamImage$       
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Pipelining two Modules}
The next step is to be able to pipeline the two main modules of the Convolution Layers and Fully Connected networks, as shown in figure \ref{fig:cnn_pipe}. When we achieve this, we will essentially gain the latency of the smallest module in total latency \ref{eq:9}.

 \begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Images/FCNN_Pipe.png} 
\decoRule
\caption[CNN Time-Chart (Arch1)]{CNN Time-Chart of Architecture v1.0}
\label{fig:cnn_pipe}
\end{figure}
 
\begin{equation}   \label{eq:9}
{
Lat_{TOTAL}=max(Lat_{CONV},Lat_{FC}+LatencyInterval)
}
\end{equation}
 
To accomplish this, major modifications have to be made to the 2 modules.

\subsubsection{Convolution Transformations}
Convolution should transform the way we export the data. For these purposes, a new entity was created by FifoToPackedFifo. This entity gets the processed image and packs it into 32 items. That's why we would need a 1024bits FIFO. This feature is not supported by HLS, so we came with a workaround by having two 512 FIFOs each containing half of the data. After the initial latency interval, every 32 * 8 = 512 cycles, it feeds the Fully Connected Layer through FIFO.

\subsubsection{Fully Connected Transformations}
With regard to the Fully Connected Layer, the way the processed image is imported should be transformed according to the above changes. Next, we need to try to gain the most out of the data that comes, or else we can drive the system into huge stalls and thus destroy the Pipeline. The way the FC has so far operated was to calculate the results for each class individually. For each class, we had to read the whole processed image again. This cannot be continued because the whole processed image will be available only when the Conv module has finished its processing. This would mean the pipeline concept will be lost. Hence we have to change the way we calculate the classes. Another dimension of parallelism needs to be added. There has to be another important change in the structure of FC. We will create 2 FC Modules where each will calculate different classes. So in 400 cycles, we will have partial results for the 800 classes. To achieve this, we need to perform a Memory Layout Transformation where it will also transform the way that weights are streamed. 


\begin{algorithm}[h]
\caption{Matrix Multiplication (Opt)}\label{alg:MAC_3}
\begin{algorithmic}[1]
\Procedure{Matrix Multiplication}{$InStream,ConvStream,input,weights$}  
\State $NumOfClasses \gets size(weights,1)$ \Comment{Number of Classes}
\State $NumOfWeights \gets size(weights,2)$ \Comment{Number of Weights/Class}
\State $FaddLat=10$ \Comment{Latency of Multiply and Accumulate}

\For{i:=1 \textbf{to} NumOfClasses }     
\State $ UNROLL$
\State $Classes(i)\gets Dense_{bias}(i)$
\EndFor 

\For{i:=1 \textbf{to} FaddLat}     
\State $ UNROLL$
\State $partialSum(i) \gets 0$
\EndFor 



\For{i:=1 \textbf{to} NumOfWeights/32 }           

\State $ConvStream.read()$

\For{j:=1 \textbf{to} NumOfClasses \textbf{step}=FaddLat}           
PIPELINE=10
\For{k:=1 \textbf{to} FaddLat} 
\State $InStream.read()$
\State $sum64=inp1*w1+inp2*w2+ ... +inp63*w63+inp64*w64$
\State $Classes[j+k]+=sum64$
\EndFor 
\EndFor 
\EndFor 

\State \textbf{return} $Classes$       
\EndProcedure
\end{algorithmic}
\end{algorithm}

In fact, a consumer-producer model has been created, where we feed at a rate of 1/256 cycles and consume at a rate of 1/
400. This could create stalls in the first module. To avoid this speech in their communication, a large FIFO has been placed

Another problem that appeared was that we had to divide the weights to 32 sets.  We possessed 64 weights and would calculate 2 classes in parallel in each module in a cycle (a total of 64 multiples in parallel). Hence the weights for each class are 28464 must be divided into 32 sets. To accomplish this, a small zero-padding of 16 elements has to be added. We also examined the method of processing them completely without using zero-padding and the rest of them sequentially but led worse results.

In table \ref{tab:pipe_fc_arch1} we present performance results after the pipeline of Convolutional Layers.


\begin{table}[h]
\caption{Pipeline Convolutional with Fully Connected Layer}
\label{tab:pipe_fc_arch1}
\centering
\begin{tabular}{l l l }
\toprule
\tabhead{Modules} & \tabhead{FLOPS} & \tabhead{MAC/cycle}   \\
\midrule
conv & 15.1M & 40(peak) \\
fc & 46.6M & 64  \\
CNN & 60.7M & 104(peak) \\
 \bottomrule\\
\end{tabular}
\begin{center}
GB=Gbytes
MAC = Multiply and Accumulate
FLOPS = Floating Point Operations
\end{center}
\end{table}

\subsection{Resource Optimizations}
It is important to ensure that we use efficiently the resources at our disposal.
\begin{itemize}
\item We used the array map where implicitly many small B-RAMs combined to create large ones while maintaining their functionality. 
\item We have noticed that HLS UNROLL was bound many Flip Flops and LUTs. These decreased considerably by manually unrolling. The tool has this behavior because it can not be guaranteed the exact number of iterations it will run. 
\item The appropriate DSPs were used through the directive resource.
\item Fixed calculations where they are performed many times we compute them once and assign them to defines variables, otherwise the tool binds resources to calculate them each time.
\end{itemize}


\begin{table}[h]
\caption{Architecture v1.0 Performance}
\label{tab:1}
\centering
\begin{tabular}{l l l l}
\toprule
\tabhead{Modules} & \tabhead{Latency} & \tabhead{Comp. Performance} &\tabhead{Bandwidth} \\
\tabhead{} & \tabhead{(cycles)} & \tabhead{(GFLOPS)} &\tabhead{(GB/s)} \\
\midrule
conv & 263685 & 17.13  & 1\\
dense & 439590 & 31.1  & 8.23\\
conv+dense & 475590 & 38.3  & 9.23\\
\bottomrule
\end{tabular}
\begin{center}
GB=Gbytes
\end{center}
\end{table}


\section{Architecture v2.0}

After the successful completion of the first architecture, we realized that there was an opening for more parallelism. So we decided to introduce another dimension of parallelism, the use of batching. Instead of computing the results for an image, we calculate for two images in parallel.

\section {Resource Optimizations}
The first thought is to insert another instance of the already existing accelerator to implement Batch 2. This would, however, lead to a doubling of resources, which would make it impossible to routing to VIVADO IDE.
Consequently, the solution is integrating batch 2 into a single architecture in HLS avoiding duplication of resources and helping the tool get better routing.

\subsection{Batching}
The weights will be streaming the same way. Instead of performing calculations for one image, we will arrange for both. The I/O will also not be increased significantly because the image is very small compared to the weights we stream in. More specifically, the I/O will grow to 0.00012\%.
The reason why we can not proceed to larger batches is that resources don't allow us.




\begin{table}[h]
\caption{Architecture v2.0 Performance}
\label{tab:1}
\centering
\begin{tabular}{l l l l}
\toprule
\tabhead{Modules} & \tabhead{Latency} & \tabhead{Comp. Performance} &\tabhead{Bandwidth} \\
\tabhead{} & \tabhead{(cycles)} & \tabhead{(GFLOPS)} &\tabhead{(GB/s)} \\

\midrule
conv & 264685 & 34.25 & 1\\
dense & 441590 & 62  & 8.23\\
conv+dense & 477590 & 76.5 & 9.23\\
\bottomrule
\end{tabular}
\begin{center}
GB=Gbytes
\end{center}
\end{table}

\section{Porting to QFDB}

Architecture 1 and 2 have been implemented as described above to run on ZCU-102. To enable the two architectures to run at QFDB, we created 4 Instances of Accelerators, ending in Batch 4 and Batch 8 respectively. Porting didn't require many transformations because there was FPGA consistency. The only substantial differences are that there is no SD on the QFDB resulting in the use of JTAG  to send the data. Finally, ARM communication with our machine was done through JTAG (instead of U-ART) using the coresight protocol.


In table \ref{tab:pipe_conv_arch1} we present performance results for the QFDB.
More specifically for the Architecture v2.0 (Batch 8).In peak performance we achieve 416 GFLOPS/s, when all modules are executed in parallel.


\begin{table}[h]
\caption{Pipeline Convolutional Layers}
\label{tab:pipe_conv_arch1}
\centering
\begin{tabular}{l l l l}
\toprule
\tabhead{Modules} & \tabhead{FLOPS} & \tabhead{MAC/cycle} & \tabhead{GFLOPS/s}   \\
\midrule
conv & 120.8 & 8 & 57.4 \\
fc & 364.8M & 16 & 104 \\
CNN & 485.6M & 832(peak) & 265 \\
 \bottomrule\\
\end{tabular}
\begin{center}
GB=Gbytes
MAC = Multiply and Accumulate
FLOPS = Floating Point Operations
\end{center}
\end{table}